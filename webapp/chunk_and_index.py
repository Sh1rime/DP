import os
import json
import math
from pathlib import Path
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss

# –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –Ω–æ—Ä–º–∞–º–∏
NORMS_DIR = Path(__file__).resolve().parent / "norms_corpus"

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ä–∞–∑–±–∏–µ–Ω–∏—è –Ω–∞ —á–∞–Ω–∫–∏
WORDS_PER_CHUNK    = 300   # —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–ª–æ–≤–∞—Ö
OVERLAP_WORDS      = 50    # –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å–æ—Å–µ–¥–Ω–∏—Ö —á–∞–Ω–∫–æ–≤


def read_txt(file_path: Path) -> str:
    """–ß–∏—Ç–∞–µ—Ç –≤–µ—Å—å —Ñ–∞–π–ª —Ü–µ–ª–∏–∫–æ–º –∫–∞–∫ —Å—Ç—Ä–æ–∫—É."""
    return file_path.read_text(encoding="utf-8")


def split_to_chunks(text: str, words_per_chunk: int, overlap: int) -> list:
    """
    –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç (—Å—Ç—Ä–æ–∫–∞) –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ N —Å–ª–æ–≤ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º overlap.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π: {'start_idx': int, 'text': str}.
    """
    # 1) –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –ø—Ä–æ–±–µ–ª–∞–º –Ω–∞ —Å–ª–æ–≤–∞
    all_words = text.split()
    total_words = len(all_words)
    chunks = []
    start = 0

    while start < total_words:
        end = start + words_per_chunk
        if end > total_words:
            end = total_words
        chunk_words = all_words[start:end]
        chunk_text = " ".join(chunk_words)
        chunks.append({
            "start_idx": start,
            "text": chunk_text
        })
        # —Å–¥–≤–∏–≥–∞–µ–º —Å—Ç–∞—Ä—Ç –Ω–∞ (WORDS_PER_CHUNK - OVERLAP_WORDS)
        if end == total_words:
            break
        start = start + (words_per_chunk - overlap)
    return chunks


def main():
    # 0) –ü—Ä–æ–≤–µ—Ä–∫–∏: –µ—Å—Ç—å –ª–∏ –≤–æ–æ–±—â–µ –ø–∞–ø–∫–∞ –∏ —Ñ–∞–π–ª—ã
    if not NORMS_DIR.exists() or not NORMS_DIR.is_dir():
        print(f"‚ùå –ü–∞–ø–∫–∞ {NORMS_DIR} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –°–æ–∑–¥–∞–π—Ç–µ –∏ –ø–æ–ª–æ–∂–∏—Ç–µ —Ç—É–¥–∞ *.txt –Ω–æ—Ä–º–∞—Ç–∏–≤—ã.")
        return

    txt_files = sorted(NORMS_DIR.glob("*.txt"))
    if not txt_files:
        print(f"‚ùå –í {NORMS_DIR} –Ω–µ—Ç txt-—Ñ–∞–π–ª–æ–≤.")
        return

    # 1) –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    print("‚è≥ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å sentence-transformers (all-MiniLM-L6-v2)‚Ä¶")
    model = SentenceTransformer("all-MiniLM-L6-v2")

    # –ë—É–¥–µ–º —Å–æ–±–∏—Ä–∞—Ç—å –≤—Å–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ –æ–¥–∏–Ω –º–∞—Å—Å–∏–≤ (float32)
    embeddings_list = []
    metadata = []  # —Å–ø–∏—Å–æ–∫ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞

    chunk_id = 0
    for txt_path in txt_files:
        print(f"‚îÄ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {txt_path.name}‚Ä¶")
        raw = read_txt(txt_path)

        # 2) –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
        chunks = split_to_chunks(raw, WORDS_PER_CHUNK, OVERLAP_WORDS)
        print(f"  ‚Ä¢ –ù–∞–π–¥–µ–Ω–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")

        # 3) –î–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞ —Å—á–∏—Ç–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
        for ch in chunks:
            text = ch["text"]
            emb = model.encode(text, convert_to_numpy=True, normalize_embeddings=True)
            embeddings_list.append(emb.astype("float32"))

            # 4) –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata.append({
                "id":        chunk_id,
                "filename":  txt_path.name,
                "start_idx": ch["start_idx"],
                "text":      text
            })
            chunk_id += 1

    if not embeddings_list:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏.")
        return

    # 5) –°–æ–±–∏—Ä–∞–µ–º –µ–¥–∏–Ω—ã–π numpy-–º–∞—Å—Å–∏–≤
    print(f"‚ÑπÔ∏è –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {len(embeddings_list)}. –°–æ–±–∏—Ä–∞–µ–º –≤ –º–∞—Ç—Ä–∏—Ü—É‚Ä¶")
    emb_matrix = np.vstack(embeddings_list)  # shape = (num_chunks, embedding_dim)

    # 6) –°–æ–∑–¥–∞—ë–º FAISS-–∏–Ω–¥–µ–∫—Å (Flat L2 —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π –≤–µ–∫—Ç–æ—Ä–æ–≤ ‚Üí –∫–æ—Å–∏–Ω—É—Å)
    dim = emb_matrix.shape[1]
    index = faiss.IndexFlatIP(dim)  # Inner Product –Ω–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö embeddings = –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
    faiss.normalize_L2(emb_matrix)
    index.add(emb_matrix)

    # 7) –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å –Ω–∞ –¥–∏—Å–∫ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    index_path = Path(__file__).resolve().parent / "norms_index.faiss"
    meta_path  = Path(__file__).resolve().parent / "norms_metadata.json"

    print(f"üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º FAISS-–∏–Ω–¥–µ–∫—Å –≤ {index_path.name} ‚Ä¶")
    faiss.write_index(index, str(index_path))

    print(f"üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ {meta_path.name} ‚Ä¶")
    with open(meta_path, "w", encoding="utf-8") as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)

    print("‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –Ω–æ—Ä–º–∞—Ç–∏–≤–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")


if __name__ == "__main__":
    main()